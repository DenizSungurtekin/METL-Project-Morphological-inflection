{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importation of libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from keras.models import Model,Sequential\n",
    "from keras.layers import Input, LSTM, Dense, SimpleRNN, Embedding\n",
    "\n",
    "# Input() is used to instantiate a Keras tensor.\n",
    "# Dense implements the operation: output = activation(dot(input, kernel) + bias) \n",
    "# LSTM : Seq to Seq model \n",
    "# Model groups layers into an object with training and inference features \n",
    "\n",
    "# from keras.preprocessing.text import one_hot\n",
    "# from keras.preprocessing.text import text_to_word_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization And Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "\n",
    "fname_train = \"data/finnish-task1-train\"\n",
    "fname_test = \"data/finnish-task1-test\"\n",
    "fname_dev = \"data/finnish-task1-dev\"\n",
    "\n",
    "train = np.loadtxt(fname_train,dtype = str)\n",
    "test = np.loadtxt(fname_test,dtype = str)\n",
    "dev = np.loadtxt(fname_dev,dtype = str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Ã¤Ã¤kkÃ¶stÃ¤Ã¤' 'pos=V,polar=POS,mood=IMP,tense=PRS,per=3,num=SG'\n",
      "  'Ã¤Ã¤kkÃ¶stÃ¤kÃ¶Ã¶n']\n",
      " ['Ã¤Ã¤kkÃ¶stÃ¤Ã¤' 'pos=V,voice=ACT,aspect=PROSP'\n",
      "  'Ã¤Ã¤kkÃ¶stÃ¤mÃ¤isillÃ¤Ã¤n']\n",
      " ['aalloittaisuus' 'pos=N,case=ON+ESS,num=PL' 'aalloittaisuuksilla']\n",
      " ...\n",
      " ['zoonoosi' 'pos=N,case=PRIV,num=SG' 'zoonoositta']\n",
      " ['zsaari' 'pos=N,case=IN+LAT,num=PL' 'zsaareihin']\n",
      " ['zumbata' 'pos=V,polar=POS,mood=POT,tense=PRS,per=2,num=PL'\n",
      "  'zumbannette']]\n",
      "(12693, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train)\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['alkeiskoppi' 'pos=N,case=NOM,num=SG' 'alkeiskoppi']\n",
      " ['lenkkitossut' 'pos=N,case=ON+ESS,num=PL' 'lenkkitossuilla']\n",
      " ['baritonitorvi' 'pos=N,case=PRIV,num=SG' 'baritonitorvetta']\n",
      " ...\n",
      " ['katkeroida' 'pos=V,polar=POS,mood=IND,tense=PRS,per=3,num=SG'\n",
      "  'katkeroi']\n",
      " ['paarmalintu' 'pos=N,case=TRANS,num=PL' 'paarmalinnuiksi']\n",
      " ['malisiÃ¶Ã¶si' 'pos=ADJ,case=IN+ABL,num=SG' 'malisiÃ¶Ã¶sistÃ¤']]\n",
      "(23633, 3)\n"
     ]
    }
   ],
   "source": [
    "print(test)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['aakkosto' 'pos=N,case=NOM,num=PL' 'aakkostot']\n",
      " ['aallottaa' 'pos=V,mood=PURP,voice=ACT' 'aallottaakseen']\n",
      " ['aaltoluku' 'pos=N,case=FRML,num=SG' 'aaltolukuna']\n",
      " ...\n",
      " ['ystÃ¤vÃ¤piiri' 'pos=N,case=ON+ABL,num=SG' 'ystÃ¤vÃ¤piiriltÃ¤']\n",
      " ['ytimekÃ¤s' 'pos=ADJ,case=ACC,num=SG' 'ytimekkÃ¤Ã¤n']\n",
      " ['zombi' 'pos=N,case=IN+ABL,num=PL' 'zombeista']]\n",
      "(1598, 3)\n"
     ]
    }
   ],
   "source": [
    "print(dev)\n",
    "print(dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Definition of encoding functions\n",
    "\n",
    "# Return the dictionary for a given list\n",
    "def list_to_dict(data):  \n",
    "    dic = {}\n",
    "    for x in data:\n",
    "        dic[x] = dic.get(x, len(dic))\n",
    "    return dic\n",
    "\n",
    "# Return the encoded array\n",
    "def encode(data):\n",
    "    dics = []\n",
    "    for i in range(data.shape[1]):\n",
    "        dic = list_to_dict(data[:,i])\n",
    "        dics.append(dic)\n",
    "        for j in range(len(data[:,i])):\n",
    "            data[:,i][j] = dic[data[:,i][j]]\n",
    "            \n",
    "    return data,dics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also store the dictionary to do the decoding operation at the end\n",
    "train,dics_train = encode(train) \n",
    "test,dics_test = encode(test)\n",
    "dev,dics_dev = encode(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train[:,0:2]\n",
    "x_test = test[:,0:2]\n",
    "x_dev = dev[:,0:2]\n",
    "\n",
    "y_train = train[:,2]\n",
    "y_test = test[:,2]\n",
    "y_dev = dev[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['0' '0']\n",
      " ['0' '1']\n",
      " ['1' '2']\n",
      " ...\n",
      " ['9853' '21']\n",
      " ['9854' '24']\n",
      " ['9855' '43']]\n",
      "(12693, 2)\n"
     ]
    }
   ],
   "source": [
    "print(x_train)\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0' '1' '2' ... '12675' '12676' '12677']\n",
      "(12693,)\n"
     ]
    }
   ],
   "source": [
    "print(y_train)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_encoder_tokens = len(x_train[:,0])\n",
    "latent_dim = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_decoder_tokens = latent_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the \n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define meta-parameter\n",
    "\n",
    "epochs = 200\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 2 array(s), but instead got the following list of 1 arrays: [array([['0', '0'],\n       ['0', '1'],\n       ['1', '2'],\n       ...,\n       ['9853', '21'],\n       ['9854', '24'],\n       ['9855', '43']], dtype='<U48')]...",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-b4ecae65cd85>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'rmsprop'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m model.fit(x_train, y_train,\n\u001b[0m\u001b[0;32m      5\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dede\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1149\u001b[0m         \u001b[1;31m# Case 2: Symbolic tensors or Numpy array-like.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1150\u001b[1;33m         x, y, sample_weights = self._standardize_user_data(\n\u001b[0m\u001b[0;32m   1151\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1152\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dede\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    572\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    573\u001b[0m         \u001b[1;31m# Standardize the inputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 574\u001b[1;33m         x = training_utils.standardize_input_data(\n\u001b[0m\u001b[0;32m    575\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    576\u001b[0m             \u001b[0mfeed_input_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dede\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'shape'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m    104\u001b[0m                 \u001b[1;34m'Error when checking model '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mexception_prefix\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m                 \u001b[1;34m': the list of Numpy arrays that you are passing to '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 2 array(s), but instead got the following list of 1 arrays: [array([['0', '0'],\n       ['0', '1'],\n       ['1', '2'],\n       ...,\n       ['9853', '21'],\n       ['9854', '24'],\n       ['9855', '43']], dtype='<U48')]..."
     ]
    }
   ],
   "source": [
    "# Run training\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
